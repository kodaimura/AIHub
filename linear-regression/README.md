# 線形回帰

線形回帰（Linear Regression）は、回帰分析の一手法で、数値データに基づいて予測するための基本的な手法です。主に、入力変数（特徴量）と出力変数（目標値）との関係が線形であると仮定し、その関係をモデル化します。

## 1. 線形回帰の基本

線形回帰では、次の数式で表される線形関係を学習します：

\[
y = w_1 x_1 + w_2 x_2 + \cdots + w_n x_n + b
\]

- \(y\) は予測値（目標変数）
- \(x_1, x_2, ..., x_n\) は入力特徴量
- \(w_1, w_2, ..., w_n\) は各特徴量に対する重み（回帰係数）
- \(b\) はバイアス項（切片）

## 2. モデルの学習方法

### 最小二乗法

線形回帰のモデルでは、最小二乗法（Least Squares Method）を使って最適なパラメータ（重みとバイアス）を学習します。この方法では、訓練データに対して予測値と実際の値の差の二乗和（誤差）を最小化します。

\[
\text{誤差関数} = \sum_{i=1}^{m} (y_i - \hat{y}_i)^2
\]

- \(m\) はデータのサンプル数
- \(y_i\) は実際の値
- \(\hat{y}_i\) は予測値

### 正規方程式

最小二乗法の誤差関数を微分し、最適なパラメータを求める方法です。正規方程式は次のように表されます：

\[
\theta = (X^T X)^{-1} X^T y
\]

- \(X\) は入力特徴量の行列
- \(y\) は実際の目標値
- \(\theta\) はパラメータベクトル（重みとバイアス）

### 勾配降下法

勾配降下法（Gradient Descent）は、誤差関数を最小化するためにパラメータを繰り返し更新する方法です。学習率\(\alpha\)を指定して、以下のように更新を行います：

\[
\theta = \theta - \alpha \nabla J(\theta)
\]

- \(\nabla J(\theta)\) は誤差関数の勾配（偏微分）

## 3. 線形回帰の評価指標

線形回帰のモデルの性能を評価するために、以下の指標がよく使われます：

### 決定係数（R²）

決定係数\(R^2\)は、モデルがどれだけ実際のデータにフィットしているかを示します。値が1に近いほど、モデルの説明力が高いとされます。

\[
R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}
\]

- \(y_i\) は実際の値
- \(\hat{y}_i\) は予測値
- \(\bar{y}\) は目標変数の平均

### 平均二乗誤差（MSE）

平均二乗誤差（Mean Squared Error, MSE）は、予測値と実際の値との差の二乗の平均です。値が小さいほど、モデルの精度が高いことを示します。

\[
\text{MSE} = \frac{1}{m} \sum_{i=1}^{m} (y_i - \hat{y}_i)^2
\]

## 4. 線形回帰の課題と拡張

### 多重共線性

多重共線性（Multicollinearity）とは、特徴量間に強い相関がある場合に、モデルのパラメータが不安定になる問題です。これを解決するためには、L1正則化（Lasso）やL2正則化（Ridge）を使うことがあります。

### 過学習と正則化

過学習（Overfitting）は、訓練データに対して過度に適合し、汎化性能が低くなる問題です。正則化（Regularization）は、モデルの複雑さを抑え、過学習を防ぐ方法です。L1正則化とL2正則化があります。

- L1正則化（Lasso回帰）
- L2正則化（Ridge回帰）