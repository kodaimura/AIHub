# 次元削減（Dimensionality Reduction）
- 高次元データを低次元に圧縮し、可視化や効率的な分析を実現

## 1. 線形次元削減法

- **主成分分析（Principal Component Analysis, PCA）**  
  - 最もよく使われる次元削減法で、データの分散が最大となる方向に新しい軸を選ぶ。
  - 高次元データを低次元に変換し、データの特徴を保持。

- **線形判別分析（Linear Discriminant Analysis, LDA）**  
  - クラス間の分散を最大化し、クラス内の分散を最小化することで、識別性能を向上させる次元削減法。
  - 主に分類問題で利用される。

- **独立成分分析（Independent Component Analysis, ICA）**  
  - PCAとは異なり、データの成分が統計的に独立であると仮定し、信号の分離を目的とする。
  - 主に信号処理などで利用される。

- **非負値行列因子分解（Non-negative Matrix Factorization, NMF）**  
  - 非負値のデータに対して、データを非負の行列に分解し、特徴量を抽出する。
  - 画像処理やテキストマイニングで使用される。

## 2. 非線形次元削減法

- **t-SNE（t-Distributed Stochastic Neighbor Embedding）**  
  - 高次元データを低次元に埋め込むための手法で、データ間の類似度を保ちながら次元を削減。
  - 主に可視化のために使用される。

- **UMAP（Uniform Manifold Approximation and Projection）**  
  - t-SNEに似た手法で、次元削減を行いながらも計算効率が高い。
  - 主に高次元データの可視化に使用される。

- **Isomap**  
  - データが多次元に埋め込まれた低次元の多様体を持つと仮定して、データ間の最短距離を保持しつつ次元削減を行う。

- **Locally Linear Embedding（LLE）**  
  - 各データ点を近傍点との線形結合で表現し、それを基に次元削減を行う。
  - 高次元データの局所的な構造を保つ。

- **自分組織化マップ（Self-Organizing Map, SOM）**  
  - ニューラルネットワークを使用して高次元データを低次元のグリッドにマッピングする。
  - 次元削減とクラスタリングを同時に行える。

## 3. その他の次元削減法

- **Autoencoders（オートエンコーダー）**  
  - ニューラルネットワークを利用して、入力データを圧縮して低次元表現に変換する。
  - 高次元データの特徴抽出や前処理に使用される。

- **Factor Analysis（因子分析）**  
  - 観測されたデータがいくつかの潜在的な因子に基づいていると仮定し、データの次元削減を行う。
  - 主に統計分析で使用される。

- **特異値分解（Singular Value Decomposition, SVD）**  
  - 行列を特異値と固有ベクトルに分解する手法で、次元削減のためにデータを再構築する。
  - 特にテキストマイニングでよく使用される。

---

# 利用が特定される状況の例

- **PCA**: 高次元データを線形的に圧縮し、重要な特徴を保持したい場合。視覚化や前処理でよく使用。
- **t-SNE / UMAP**: 高次元データの可視化が必要な場合に使用。特にクラスタリング結果を視覚化する際に有効。
- **LDA**: 主に分類タスクで、クラス間の分散を最大化して次元削減を行いたい場合。
- **Autoencoders**: 高次元のデータを圧縮して、次元削減と特徴抽出を行いたい場合。深層学習を使用したデータ処理に便利。
- **因子分析 / SVD**: 特にテキストや信号処理などのデータの分解を行いたい場合。
